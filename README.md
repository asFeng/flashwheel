# prebuilt FlashAttention wheels

### Python **cp312** (Python 3.12)

```bash
# Torch 2.6.0a0+df5bbc09d1.nv24.11 / CUDA 12.6 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.6.0a0+df5bbc09d1.nv24.11-cu12.6-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.6.0a0+df5bbc09d1.nv24.12 / CUDA 12.6 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.6.0a0+df5bbc09d1.nv24.12-cu12.6-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.6.0a0+ecf3bae40a.nv25.01 / CUDA 12.8 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.6.0a0+ecf3bae40a.nv25.01-cu12.8-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.7.0a0+ecf3bae40a.nv25.02 / CUDA 12.8 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.7.0a0+ecf3bae40a.nv25.02-cu12.8-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.7.0a0+7c8ec84dab.nv25.03 / CUDA 12.8 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.7.0a0+7c8ec84dab.nv25.03-cu12.8-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.7.0a0+79aa17489c.nv25.04 / CUDA 12.9 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.7.0a0+79aa17489c.nv25.04-cu12.9-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.8.0a0+5228986c39.nv25.05 / CUDA 12.9 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.8.0a0+5228986c39.nv25.05-cu12.9-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.8.0a0+5228986c39.nv25.06 / CUDA 12.9 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.8.0a0+5228986c39.nv25.06-cu12.9-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"

# Torch 2.8.0a0+34c6371d24.nv25.08 / CUDA 13.0 / cp312
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.8.0a0+34c6371d24.nv25.08-cu13.0-cp312/flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl"
```

---
### Python **cp310** (Python 3.10)

```bash
# Torch 2.3.0a0+40ec155e58.nv24.03  / CUDA 12.4  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.3.0a0+40ec155e58.nv24.03-cu12.4-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.3.0a0+6ddf5cf85e.nv24.04  / CUDA 12.4  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.3.0a0+6ddf5cf85e.nv24.04-cu12.4-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.3.0a0+ebedce2            / CUDA 12.3  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.3.0a0+ebedce2-cu12.3-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.4.0a0+07cecf4168.nv24.05 / CUDA 12.4  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.4.0a0+07cecf4168.nv24.05-cu12.4-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.4.0a0+f70bd71a48.nv24.06 / CUDA 12.5  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.4.0a0+f70bd71a48.nv24.06-cu12.5-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.4.0a0+3bcc3cddb5.nv24.07 / CUDA 12.5  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.4.0a0+3bcc3cddb5.nv24.07-cu12.5-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.5.0a0+872d972e41.nv24.08 / CUDA 12.6  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.5.0a0+872d972e41.nv24.08-cu12.6-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.5.0a0+b465a5843b.nv24.09 / CUDA 12.6  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.5.0a0+b465a5843b.nv24.09-cu12.6-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"

# Torch 2.5.0a0+e000cf0ad9.nv24.10 / CUDA 12.6  / cp310
pip install "https://github.com/asFeng/flashwheel/releases/download/fa-2.8.3-torch2.5.0a0+e000cf0ad9.nv24.10-cu12.6-cp310/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl"
```